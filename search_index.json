[["index.html", "PO12Q: Seminar Companion Preface", " PO12Q: Seminar Companion Dr Flo Reiche Department of Politics and International Studies University of Warwick Last Updated 06 December, 2024 Preface This is the online companion to the seminars on PO12Q. This replaces the physical worksheets which we work through in seminars for two reasons: it is environmentally friendlier you can copy / paste code directly from the code chunks in this online companion, whereas doing so from a pdf (on Moodle) tends to lead to problems with character encoding. I hope you find this useful! "],["worksheet-week-1.html", "Worksheet Week 1 Self-Assessment Questions1 Calculations by Hand Cross-Tabulations in R Solutions", " Worksheet Week 1 Self-Assessment Questions1 What do cross-tabulations do? Can we use continuous variables for cross-tabulations? What are the strengths and weaknesses of cross-tabulations? Why do we calculate the \\(\\chi^2\\)-value as \\(\\chi^{2} = \\Sigma \\frac{(f_{o}-f_{e})^{2}}{f_{e}}\\) ? How does the \\(\\chi^2\\) distribution differ from the t- and normal distribution? Calculations by Hand I have given you an example of a cross-tabulation in the lecture. Consider the following Table: Calculate the Expected Values and fill in the following table: Calculate the \\(\\chi^{2}\\)-value How many degrees of freedom does this table have? Why? Using the \\(\\chi^2\\) Table, what is the p-value? Are mode of transport and year of study independent in the population? Cross-Tabulations in R Data Set We are working with the World Development Indicators this week, data are taken from World Bank (2020) and Marshall &amp; Gurr (2020). Download the data WDI_PO12Q.csv and place it in a new working directory for this week The variables we will be working with today are as follows: Loading the Data Guided Example We are now going to use the WDI, and produce a crosstab of Life expectancy at birth, total (years) by GDP per capita (constant 2010 US$), using the variables and We first need to recode both variables into factor variables, as these are continuous. First up is Recoding: as a refresher from PO11Q, we are specifying the new data frame as the old one, then we add a pipe, and call the function . Therein, we create a new variable called , which will be an ordered factor, cutting the variable at the stated intervals, and labeling these levels accordingly. Next up is which we are recoding into a factor variable with three levels: Let us now see whether the level of GDP has an influence on life expectancy State the null and the alternative hypothesis (directional) Run the cross-tabulation, by calling: lifecat gdpcat low high low 69 0 medium 54 16 high 3 27 Now perform a chi-squared test, using the command2 Pearson&#39;s Chi-squared test data: wdi_table X-squared = 89.702, df = 2, p-value &lt; 2.2e-16 Are the results statistically significant? At what level? Produce the table of expected frequencies, using the command round(Xsq$expected, 2) lifecat gdpcat low high low 51.44 17.56 medium 52.19 17.81 high 22.37 7.63 You can compare that to the observed ones: round(Xsq$observed, 2) lifecat gdpcat low high low 69 0 medium 54 16 high 3 27 You can see that this is identical to producing the cross-tabulation in the first place: lifecat gdpcat low high low 69 0 medium 54 16 high 3 27 What have we found out? Exercises Let us find out whether the completion of primary school influences unemployment rates. State the null and directional alternative hypothesis for this test. Create a new variable primary_fac using the prim_compl variable. Cut it into three categories “low”, “medium”, and “high”, cutting prim_compl at its first quartile, and its mean. Apply the same procedure to unemploy, creating a new variable called unemp_fac. Create a cross-tabulation assessing the dependence of youth unemployment on primary completion rate. unemp_fac. Test whether the dependence is statistically significant. unemp_fac. Repeat steps 1a) to 1e) for two variables of your own choice. Solutions You can find the Solutions in the Downloads Section. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ I am using the option here to reproduce the \\(\\chi^{2}\\)-value you would get if you calculated this by hand. Technically, \\(\\chi^{2}\\) is only an approximation of the hypergeometric distribution which would deliver an exact test. You can get the precise value by applying Yates’ continuity correction with .↩︎ "],["worksheet-week-2.html", "Worksheet Week 2 Self-Assessment Questions3 Two-Sample Tests in R Solutions", " Worksheet Week 2 Self-Assessment Questions3 Give an example for a two-sample test for a mean. Give an example for a two-sample test for a proportion. Why do we calculate the t-score as \\(t =\\frac{\\text{Estimate of parameter - null hypothesis value of parameter}}{\\text{Standard error of estimate}}\\) ? What is the difference between a t-score and a z-score? What are the strengths and weaknesses of two-sample tests? Two-Sample Tests in R Data Preparation We are working with the World Development Indicators again. Data are taken from World Bank (2020), Boix et al. (2018), and Marshall &amp; Gurr (2020). To save you rooting in your files, here is the codebook (you are welcome): Load the data set The Polity V Score (variable polity5) codes regimes from -10 (indicating perfect autocracy) to +10 (indicating perfect democracy). With the tidyvserse, create a new variable called democracy which codes all countries with a Polity V score lower than +1 as dictatorships, and all countries with a Polity V score from +1 to +10 as democracies. Apply the same procedure to gdp, cutting it at its median into two levels, Developing and Developed, creating a new variable called gdpcat. Last up is the variable gdpgrowth. Create a new variable called growth which divides countries into “slow-growing” and “fast-growing” countries, using the mean as the cut-off point. Guided Example – Two-Sample Test for a Proportion Let us find out whether a higher proportion of developed countries is democratic than developing countries. State the null hypothesis and the directional alternative hypothesis for this research question. In order to test this hypothesis, we need to first create a cross-tabulation: Dictatorship Democracy Developing 21 36 Developed 19 71 We now take the number of observations which are classed as democracies per development status. We also calculate the row totals, as this gives us the total number of developing and developed countries, respectively. Then we are ready to use the prop.test() command, by first specifying the number of countries which are democracies, then the total number of developing and developed countries, then advising R that a correction for small sample sizes is not necessary in our case. Our hypothesis is directional, because we expect a higher proportion of developed countries to be democratic than developing countries. The status Developing is the lower category, and we thus expect this proportion to be smaller, or “less”. We add this to the test function as option alternative = \"less\". 2-sample test for equality of proportions without continuity correction data: c(36, 71) out of c(57, 90) X-squared = 4.3602, df = 1, p-value = 0.01839 alternative hypothesis: less 95 percent confidence interval: -1.00000000 -0.03061662 sample estimates: prop 1 prop 2 0.6315789 0.7888889 Which proportion of developing and developed countries are democratic? Do we verify or falsify our hypothesis at a 95% confidence level? Exercise – Two-Sample Test for a Proportion Is a higher proportion of fast-growing countries democratic than slow-growing countries? Use a 95% confidence level. Guided Example – Two-Sample Test for a Mean Now we are interested whether people live longer in developed countries than in developing countries? For this, we use the variable life Once again, state the Null- and the Alternative Hypothesis The t-test assumes an equal variance in both samples, and so we need to test whether this is the case. We will do this with the so-called Levene Test, where: \\(H_{0}\\): The variance among the groups is equal. \\(H_{A}\\): The variance among the groups is not equal. For the Levene test you need the package, where “car” stands for “Companion to Applied Regression”: Now we call the package and perform the test: Levene&#39;s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 1 3.7671 0.05395 . 167 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The result is insignificant, and we therefore accept the null hypothesis. This means that the variance in the two samples is equal. Now we can perform the t-test. We once again specify alternative=\"less\" as an option, due to the same reasoning as before. Two Sample t-test data: lifeexp by gdpcat t = -12.01, df = 167, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means between group Developing and group Developed is less than 0 95 percent confidence interval: -Inf -9.53028 sample estimates: mean in group Developing mean in group Developed 64.65938 75.71177 Can we conclude at a 95% confidence level, that people live longer in developed countries than in developing countries? Exercise – Two-Sample Test for a Mean Do people live longer under democracies than under dictatorahips (use the politybin variable)? Use a 95% confidence level. Solutions You can find the Solutions in the Downloads Section. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["introduction-to-matrices.html", "Introduction to Matrices What is a Matrix?4 Matrix Notation Calculating with Matrices Special Matrices", " Introduction to Matrices What is a Matrix?4 The word “matrix” generally conjures up levels of horror in students which even Stephen King would be struggling to match5. And I have to admit that I have not always been best friends with them myself, either. But they are useful, and in their ability to convey a large amount of information in a structured and logical way, they are even beautiful. Because at the end of the day, a matrix is nothing more than a list. In the lecture I introduced you to the concept of the Population Regression Function (PRF) which can be written as: \\[\\begin{equation} y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i} \\end{equation}\\] If we wanted to write out this equation for every observation in our data set, it would look something like this: \\[\\begin{align*} y_{1} &amp;= \\beta_{0} + \\beta_{1} x_{1} + \\epsilon_{1} \\\\ y_{2} &amp;= \\beta_{0} + \\beta_{1} x_{2} + \\epsilon_{2} \\\\ &amp;\\vdots \\\\ y_{n} &amp;= \\beta_{0} + \\beta_{1} x_{n} + \\epsilon_{n} \\end{align*}\\] This is incredibly wasteful, as it unnecessarily repeats notation. What we could do instead is to create a list which has as many columns as there are unique elements in these equations (since they all have the same structure), and only note the numeric values which actually change. And there you have a matrix: Whilst this is what we will be using matrices for next week, let us leave regression aside for now, and focus on working with matrices more generally. We will now be using these two sample matrices: \\[\\begin{equation*} A = \\begin{bmatrix} 1 &amp; 7 &amp; 3\\\\ 9 &amp; 5 &amp; 4\\\\ \\end{bmatrix} \\hspace{0.75cm} B = \\begin{bmatrix} 6 &amp; 3 &amp; 8 &amp; 2 \\\\ 3 &amp; 2 &amp; 1 &amp; 4 \\\\ 1 &amp; 5 &amp; 3 &amp; 9 \\\\ \\end{bmatrix} \\end{equation*}\\] Matrix Notation We can refer to individual elements of a matrix by stating the name of the matrix, and then in the index first the row, followed by the column (you might recognise this logic from R – this is because it is the same). With respect to matrix A, for example, the value in the first row and first column (1) would be referred to as \\(A_{11}=1\\) The number in the first row, but second column would be referred to as \\(A_{12}=7\\) Calculating with Matrices We will only need to multiply and divide matrices on this module, so let’s cover these operations now. Multiplying Matrices To show you how this is done, I will multiply matrix A with matrix B and record the results in a new matrix called C. \\[\\begin{equation*} A \\times B = C = \\begin{bmatrix} 1 &amp; 7 &amp; 3\\\\ 9 &amp; 5 &amp; 4\\\\ \\end{bmatrix} \\times \\begin{bmatrix} 6 &amp; 3 &amp; 8 &amp; 2 \\\\ 3 &amp; 2 &amp; 1 &amp; 4 \\\\ 1 &amp; 5 &amp; 3 &amp; 9 \\\\ \\end{bmatrix} = \\begin{bmatrix} 30 &amp; 32 &amp; 24 &amp; 51\\\\ 73 &amp; 57 &amp; 89 &amp; 74\\\\ \\end{bmatrix} \\end{equation*}\\] In order to calculate a new element \\(C_{i,j}\\), we multiply the elements of the \\(i^{th}\\) row of A with the elements of the \\(j^{th}\\) column of B. We then add together these so-called inner products in order to arrive at \\(C_{i,j}\\). Let me give you some examples in which I have set the values from matrix A in bold to make the process more transparent. \\(C_{11}=\\textbf{1}\\times6+\\textbf{7}\\times3+\\textbf{3}\\times1=30\\) \\(C_{12}=\\textbf{1}\\times3+\\textbf{7}\\times2+\\textbf{3}\\times5=32\\) \\(C_{21}=\\textbf{9}\\times6+\\textbf{5}\\times3+\\textbf{4}\\times1=73\\) I have prepared a short video taking you through this process step by step. I would encourage you to watch it now. You might have to sign in with your university credentials. Dividing Matrices In order to be able to divide by an entire matrix, we take its inverse, and then multiply with the inverse. We do the same in the non-matrix world. For example, if we wanted to divide 6 by 3, this is the same as multiplying 6 with \\(\\frac{1}{3}\\), the inverse of 3. Sadly, inverting a matrix is not quite as straightforward as this. In fact, it is one of the most challenging operations you can do with a matrix. Luckily, the inversion of a 2 by 2 matrix (which is what we will be using) is still possible without a degree in algebra. The inverse \\(D^{-1}\\) of a 2 by 2 matrix \\(D\\) is defined as \\[\\begin{equation*} D^{-1} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\\\ \\end{bmatrix}^{-1} \\end{equation*}\\] Thus, to arrive at the inverse of a 2 by 2 matrix, we first have to form the fraction in front of it. This takes as its denominator the difference between the products of the diagonal elements. We also refer to the denominator as the determinant of the matrix. In a second step – now in the matrix itself – we swap a and d, and set a minus sign in front of b and c. Once again, there is a video on this: Special Matrices There are two types of matrices we will be using which hold special, useful properties. This is the transpose of a matrix, and the so-called identity matrix. Transposing Matrices Another important operation is transposing a matrix, which turns rows into columns and columns into rows. We denote a transposed matrix with an apostrophe. Transposing matrix \\(A\\) into matrix \\(A^\\prime\\) gives us: \\[\\begin{equation*} A = \\begin{bmatrix} 1 &amp; 7 &amp; 3\\\\ 9 &amp; 5 &amp; 4\\\\ \\end{bmatrix} \\hspace{0.75cm} A^\\prime = \\begin{bmatrix} 1 &amp; 9 \\\\ 7 &amp; 5 \\\\ 3 &amp; 4 \\\\ \\end{bmatrix} \\end{equation*}\\] You guessed it, here is a video on transposing matrices: The Identity Matrix There is only one last thing left to show you before we can embark on using matrices for deriving and estimating our regression coefficients. And that is the so-called identity matrix \\(I\\). This matrix is always square, has the value 1 on all diagonal elements, and zeros otherwise. If a matrix is multiplied with \\(I\\), we receive the original matrix. For example let \\[\\begin{equation*} I = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\\\ \\end{bmatrix} \\end{equation*}\\] If we multiply I with matrix B we receive \\[\\begin{equation*} I \\times B = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\\\ \\end{bmatrix} \\times \\begin{bmatrix} 6 &amp; 3 &amp; 8 &amp; 2 \\\\ 3 &amp; 2 &amp; 1 &amp; 4 \\\\ 1 &amp; 5 &amp; 3 &amp; 9 \\\\ \\end{bmatrix} = \\begin{bmatrix} 6 &amp; 3 &amp; 8 &amp; 2 \\\\ 3 &amp; 2 &amp; 1 &amp; 4 \\\\ 1 &amp; 5 &amp; 3 &amp; 9 \\\\ \\end{bmatrix} \\end{equation*}\\] This feature will be important in the derivation of estimators next week, where we will make use of the fact that a matrix multiplied with its inverse results in an identity matrix. For example \\(A \\times A^{-1} = I\\). You can check this process out in this video: This material is taken from Reiche (forthcoming).↩︎ He is by far my favourite author. If you haven’t, already, read IT.↩︎ "],["worksheet-week-3.html", "Worksheet Week 3 Self-Assessment Questions6 Regression – Theory Calculations with Matrices Homework", " Worksheet Week 3 Self-Assessment Questions6 Explain the difference between \\(\\hat{y}_i\\) and \\(y_i\\). Give an example for a scenario in which you could use regression analysis. Why is there an error term in regression? How does an error term differ from a residual? Consider the vector of error terms \\(\\epsilon\\) (an n \\(\\times\\) 1 matrix). How do you write \\(\\sum \\epsilon^2\\) in matrix notation? Regression – Theory You are given the scatter plot in Figure 2.7 (taken from Gujarati &amp; Porter (2009)) along with the regression line. What general conclusion do you draw from this diagram? Is the regression line sketched in the diagram a population regression line or a sample regression line? Calculations with Matrices As indicated on Moodle, we will start working with matrices next week. To familiarise yourself with these and to get a better overview how to work with them, please work through the following exercises. Feel free to consult the “Introduction to Matrices” document on Moodle for this. First, answer these questions: - What is a matrix? - What does transposition do? - What is the purpose of an identity matrix? Now, consider the following three matrices A, B, and C: \\[\\begin{equation*} A = \\begin{bmatrix} 9 &amp; 4 &amp; 11 \\\\ 6 &amp; 8 &amp; 3 \\\\ 14 &amp; 7 &amp; 9 \\\\ \\end{bmatrix} \\hspace{1cm} B = \\begin{bmatrix} 13 &amp; 8 &amp; 12 &amp; 2 \\\\ 1 &amp; 5 &amp; 15 &amp; 3 \\\\ \\end{bmatrix} \\hspace{1cm} C = \\begin{bmatrix} 25 &amp; 22 \\\\ 31 &amp; 19 \\\\ \\end{bmatrix} \\end{equation*}\\] . What is the value in: Transpose \\(B\\) into \\(B^\\prime\\) using the following blank matrix. \\[\\begin{equation*} B^\\prime = \\begin{bmatrix} &amp; &amp; &amp; &amp;\\\\ &amp; &amp; &amp; &amp;\\\\ &amp; &amp; &amp; &amp;\\\\ &amp; &amp; &amp; &amp;\\\\ \\end{bmatrix} \\end{equation*}\\] Solve \\(D\\) where \\(C \\times B = D\\) using the following blank matrix. \\[\\begin{equation*} D = \\begin{bmatrix} &amp; &amp; &amp; &amp; &amp; &amp;\\\\ &amp; &amp; &amp; &amp; &amp; &amp;\\\\ &amp; &amp; &amp; &amp; &amp; &amp;\\\\ &amp; &amp; &amp; &amp; &amp; &amp;\\\\ \\end{bmatrix} \\end{equation*}\\] Solve \\(C^{-1}\\), showing your workings using the appropriate formula. \\[\\begin{equation*} C^{-1} = \\frac{\\:}{\\hspace{2.5cm}} \\begin{bmatrix} &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\\\ \\end{bmatrix} \\hspace{2.5mm}=\\hspace{2.5mm} \\begin{bmatrix} &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\\\ \\end{bmatrix} \\end{equation*}\\] Homework Familiarise yourself further with matrices by setting yourself two sample matrices, A and B (each should be a 2x2 matrix). Conduct the following operations (you can check your solutions with https://matrixcalc.org/en/): Multiply A and B Invert A and B Transpose A and B Multiply A with an Identity Matrix Multiply A with A\\(^{-1}\\). What is the result called? Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["worksheet-week-4.html", "Worksheet Week 4 Self-Assessment Questions7 Regression – Calculations Regression in R Solutions", " Worksheet Week 4 Self-Assessment Questions7 How does OLS fit the regression line? Why do we have to square the residuals for OLS? What are the advantages of working with matrices in regression analysis? Explain the concept of an identity matrix. Regression – Calculations Consider the following data set: Plot the data in Table \\(\\ref{tbl:data1}\\) in a suitable scatter plot. Fit a line of best fit through the scatterplot. Assuming a regression model of the type \\(Y_{i}=\\beta_{0}+ \\beta_{1}X_{i}+\\epsilon_{i}\\), calculate the estimators for \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Calculate the regression coefficients \\(\\beta_{0}\\) and \\(\\beta_{1}\\) using matrices. Specify the SRF and interpret the estimators of \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Regression in R Load the data set from Excel. Run the regression in R as follows: regression &lt;- lm(income ~ age, data = incomedata) summary(regression) Call: lm(formula = income ~ age, data = incomedata) Residuals: Min 1Q Median 3Q Max -652.25 -102.92 6.53 142.21 584.39 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -53.092 304.716 -0.174 0.866011 age 39.695 7.325 5.419 0.000631 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 335.3 on 8 degrees of freedom Multiple R-squared: 0.7859, Adjusted R-squared: 0.7592 F-statistic: 29.37 on 1 and 8 DF, p-value: 0.0006314 Check your results from the first section. Interpret the coefficients. Solutions You can find the Solutions in the Downloads Section. Some of the content of this worksheet is taken from .↩︎ "],["worksheet-week-5.html", "Worksheet Week 5 Self-Assessment Questions8 Mathematical Properties of OLS Goodness of Fit – Using R Goodness of Fit – By Hand Solutions", " Worksheet Week 5 Self-Assessment Questions8 Why do we need a measure to assess goodness of fit? How do you interpret R-Squared? Explain in your own words what the residual sum of squares (RSS) means. Explain one of the mathematical properties of OLS in your own words. Mathematical Properties of OLS I showed in the lecture that the predicted values of y are uncorrelated with the residuals. Show mathematically that this is not the case for the error terms. (Hint: Consider carefully Equation 2 on Slide 8.) Goodness of Fit – Using R We will be using our WDI example again to explore model fit in R. Data are taken from World Bank (2020) and Marshall &amp; Gurr (2020). The data set is available on Moodle. Set your working directory and load the data set as an object called wdi. We will re-assess our question from Week 1, whether the level of GDP has an influence on life expectancy State the null and the alternative hypothesis (directional) Run the regression model. Build the SRF and interpret the coefficients. Assess and interpret model fit, using “Multiple R-squared” which is equivalent to the R\\(^2\\) we discussed in the lecture. wdi &lt;- read.csv(&quot;files/Week 5/WDI_PO12Q.csv&quot;) attach(wdi) model &lt;- lm(lifeexp ~ gdppc) summary(model) Call: lm(formula = lifeexp ~ gdppc) Residuals: Min 1Q Median 3Q Max -17.445 -3.260 1.545 4.760 8.969 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.815e+01 5.797e-01 117.56 &lt;2e-16 *** gdppc 2.619e-04 2.515e-05 10.41 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 6.129 on 167 degrees of freedom (26 observations deleted due to missingness) Multiple R-squared: 0.3936, Adjusted R-squared: 0.39 F-statistic: 108.4 on 1 and 167 DF, p-value: &lt; 2.2e-16 You can also extract specific blocks of the output table by placing brackets [] after the summary() function. For example summary()[1]. Try to extract the block containing model fit, like this: $r.squared [1] 0.3936356 Goodness of Fit – By Hand Using the data and calculations presented in the Table below, calculate the coefficient of determination, \\(r^{2}\\), with \\(\\hat{Y_{i}}= \\widehat{-53.1} + \\widehat{39.7} X_{i}\\) Solutions You can find the Solutions in the Downloads Section. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["reading-week.html", "Reading Week", " Reading Week "],["worksheet-week-7.html", "Worksheet Week 7 Self-Assessment Questions9 Regression – Standard Errors of Coefficients Regression – Hypothesis Testing &amp; Confidence Intervals Solutions", " Worksheet Week 7 Self-Assessment Questions9 Why do we need to calculate standard errors of estimators? Describe the relationship between confidence intervals and hypothesis testing. What does \\(\\sigma^2\\) represent substantively? Regression – Standard Errors of Coefficients Using the following regression calculations, determine the size of the standard errors of \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) in tabular form in matrix form Regression – Hypothesis Testing &amp; Confidence Intervals Consider the following regression, where gdp indicates Gross Domestic Product (PPP) in 2005 US Dollars, and life indicates life expectancy at birth in years. I am running the regression with the first line and store the results in the object wdi. The second line asks R to display the detailed results for the regression. Data are taken from World Bank (2020) and Marshall &amp; Gurr (2020). model1 &lt;- lm(gdppc ~ lifeexp, data = wdi) summary(model1) Call: lm(formula = gdppc ~ lifeexp, data = wdi) Residuals: Min 1Q Median 3Q Max -18457 -9877 -4187 4963 78242 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -94306.8 10406.7 -9.062 3.33e-16 *** lifeexp 1503.2 144.4 10.412 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14690 on 167 degrees of freedom (26 observations deleted due to missingness) Multiple R-squared: 0.3936, Adjusted R-squared: 0.39 F-statistic: 108.4 on 1 and 167 DF, p-value: &lt; 2.2e-16 Build the regression function and interpret the coefficients. Plot the regression function in a suitable diagram using ggplot. Explain how the t-value for life is obtained. What does the value of “Multiple R-Squared” (this is equivalent to the R-Squared we calculated by hand last week) mean? Calculate the 95% confidence intervals for the coefficient life and the intercept. Compare your results to the R output below. Find two explanations in the output for why the coefficient for life is statistically significant at the 5% level? confint(model1,level = 0.95) 2.5 % 97.5 % (Intercept) -114852.512 -73761.11 lifeexp 1218.183 1788.24 Solutions You can find the Solutions in the Downloads Section. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["additional-exercises.html", "Additional Exercises Load Packages and data10 Inspect your data Preliminary Analysis Visualisation Visualisation 2.0 Saving the Scatterplot Regression Analysis (yes, finally) Interpretation Exporting the Results Comparing models Solutions", " Additional Exercises Load Packages and data10 Before starting, we need to load libraries and install packages if not already installed. In these exercises we will be using the following packages: haven ggplot2 stargazer Set working directory and load the data. The file is on Moodle. It’s called simd.csv and it’s the “Scottish Index of Multiple Deprivation”. More details here: https://www.gov.scot/collections/scottish-index-of-multiple-deprivation-2020/ Hint: Remember that the WD is the folder where you stored the data set and where all the outputs will be saved Hint2: Note, the data is a .csv file Inspect your data Here you can use several basic functions. The dataset does not contain too many variables, so you can start by using names(), str(), dim(), etc. Preliminary Analysis Now that you have a preliminary idea of the structure of the dataset, you can select two variables and test a possible relationship. The topic today is bivariate linear regression analysis, so remember that the outcome variable needs to be continuous. Let’s say we want to look at the relationship between alcohol consumption and mortality rate (yes, not a very funny topic, but interesting nonetheless). The two variables are, respectively, alcohol and mortality (rather intuitive this time). Now, formulate the working (alternative) and the null hypothesis. Write them down. H\\(\\pmb{_0}\\): H\\(\\pmb{_1}\\): Which is your dependent variable? Run a frequency table on the mortality variable. What is the level of measurement? Do the same for the other variable. And guess what is the level of measurement. Visualisation Let’s start with the visualisation of the relationship between the two variables. What is the best way to visualise the relationship considering the level of measurement of our variables? Hint: Probably a scatterplot, right? So, use a scatterplot to visualise the relationship and add the regression line. You can use ggplot, but also the standard plot() function. Improve the graph by: Adding a regression line. Adding up a relevant title, also possibly a subtitle. Adding axes labels and making them readable. Remove the grid in the background. If you have done everything correctly, you should see that the dots are rather concentrated in the bottom left corner of the scatterplot with some outliers far away from the cloud of our data. It is not a big deal, but we might want to get rid of the outliers and this way improve our visualisation. There are several ways to do that, of course. But let’s say we want to transform our variables, excluding all values over a certain point. For instance, we want to exclude the values above 750 of our alcohol variable and above 500 for our mortality variable. How would you do that? Hint: there are several solutions. One could be, creating a new datset subsetting the original. Another solution could be again, creating a new variable telling R to transform all the values above our treshold in NA (missing values). Try to find an apply the appropriate code. Use Google if necessary, it helps a lot. Visualisation 2.0 Now, visualise the relationship using the new variables. What can you see? You can improve the scatterplot using a series of arguments (e.g., alpha) in the geom_point() function in ggplot. Try to improve the Aesthetics of the scatterplot playing with alpha, for instance. (see: https: //www.rdocumentation.org/packages/ggplot2/versions/3.4.0/topics/geom_point). Also, you can draw a vertical and horizontal line corresponding to the mean of your variables using geom_hline and geom_vline. You can thus check if the regression line passes through the mean of X and Y. (see: https://www.rdocumentation.org/packages/ggplot2/versions/0.9.1/topics/geom_hline). Saving the Scatterplot You can also save a graph as .png, .JPG (even .pdf) that you can then import in a word document. Although there are many way to use your R output, saving a graph might be sometimes useful. Use the function ggsave() to save your scatterplot. Again, there are tons of examples online, google it. Hint: You first need to store the graph in an object. Hint 2: The file will end up in your working directory Regression Analysis (yes, finally) Now we can finally run a linear regression with mortality as the outcome variable and alcohol as the predictor using the lm() function. Store the results in an object called model and visualise the regression output using summary(). Use the reduced data set without outliers. Call: lm(formula = mortality ~ alcohol, data = simred) Residuals: Min 1Q Median 3Q Max -122.69 -22.52 -4.71 17.27 377.77 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 78.155160 0.619648 126.13 &lt;2e-16 *** alcohol 0.208270 0.004476 46.53 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 35.61 on 6956 degrees of freedom Multiple R-squared: 0.2374, Adjusted R-squared: 0.2373 F-statistic: 2165 on 1 and 6956 DF, p-value: &lt; 2.2e-16 You can also extract specific blocks of the output table. One way of doing it is to use the brackets [] after the summary() function. For example summary()[8]. Try to extract the block of Coefficients from the table, like this: $coefficients Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 78.1551599 0.619648419 126.12823 0 alcohol 0.2082697 0.004476011 46.53021 0 Interpretation Interpret the results, starting with model evaluation. Is the p-value of the F-statistics statistically significant? We will be discussing this in our following lectures. How much variation in the outcome variable does the model explain? What does this tell us about the model? What’s the value of the slope? What does it mean? What’s the value of the regression coefficient? How do we interpret it? Is it statistically significant? What does it mean in practice? Interpret the results (in plain language) referring to the hypothesis you formulated above. Exporting the Results As for the graphs, you can also export and save the results of the regression model in a Word table. To do that you can use the stargazer package. Try and export the table. Hint 1: The function needs to contain, in order: the name of the R object where you stored the regression results, the option header=F to suppress the annoying immortalisation of the author, the option type=\"html\", and the option out=\"documentname.doc\" which places a word document with that file name in your working directory. Hint 2: Remember that Google is your best friend when you learn to code. And after that as well. No one actually remembers all the functions and all their argument. The secret is to know how to google the things you need. You can improve the table in many ways. For example, you need to replace the variable names with the variable labels, You could add a name of the model and suppress unneeded statistics (see https://www.rdocumentation.org/packages/stargazer/versions/5.2.3/topics/stargazer_stat_code_list). Comparing models You can now run another regression model with a different independent variable. Can you compare your original model with the new one? How? How do you know which independent variable is doing a better job in explaining your dependent variable? Solutions You can find the Solutions in the Downloads Section. These exercises were originally written by Oleksiy Bondarenko who taught the module in 2022/23. I have slightly altered them in subsequent years.↩︎ "],["worksheet-week-8.html", "Worksheet Week 8", " Worksheet Week 8 "],["worksheet-week-9.html", "Worksheet Week 9", " Worksheet Week 9 "],["worksheet-week-10.html", "Worksheet Week 10", " Worksheet Week 10 "],["downloads.html", "Downloads 0.1 Documents 0.2 Data Sets 0.3 R Scripts", " Downloads 0.1 Documents PO12Q Bibliography Statistical Tables Week 1 Worksheet Solutions Week 2 Worksheet Solutions Week 4 Worksheet Solutions Week 4 Regression Calculations Week 5 Regression Calculations Week 7 Worksheet Solutions Week 7 Regression Calculations Week 7 Solutions to Additional Exercises 0.2 Data Sets WDI_PO12Q.csv PO12Q_4.xslx 0.3 R Scripts Week 2 R Solutions "],["bibliography.html", "Bibliography", " Bibliography Boix, C., Miller, M., &amp; Rosato, S. (2018). Boix-Miller-Rosato Dichotomous Coding of Democracy, 1800-2015 (Version V3). Harvard Dataverse. Gujarati, D. N., &amp; Porter, D. C. (2009). Basic Econometrics (Fifth International Edition). New York: McGraw-Hill. Marshall, M. G., &amp; Gurr, T. R. (2020). Polity V Project: Political Regime Characteristics and Transitions, 1800-2018. available online at http://www.systemicpeace.org/inscrdata.html. Reiche, F. (forthcoming). Introduction to Quantitative Methods for the Social Sciences. Oxford: Oxford University Press. World Bank. (2020). World Development Indicators. available online at https://datacatalog.worldbank.org/dataset/world-development-indicators. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
